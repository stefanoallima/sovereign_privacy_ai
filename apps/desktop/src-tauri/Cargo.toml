[package]
name = "ailocalmind"
version = "0.1.0"
description = "Privacy-first AI desktop assistant with local LLM, PII encryption, and hybrid cloud routing"
authors = ["AILocalMind Contributors"]
license = "MIT"
repository = "https://github.com/your-username/AILocalMind"
homepage = "https://github.com/your-username/AILocalMind"
keywords = ["privacy", "ai", "desktop", "tauri", "ollama"]
edition = "2021"

[lib]
name = "private_assistant_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = ["tray-icon"] }
tauri-plugin-opener = "2"
tauri-plugin-global-shortcut = "2"
tauri-plugin-shell = "2"
tauri-plugin-dialog = "2"
tauri-plugin-updater = "2"
tauri-plugin-process = "2"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
rusqlite = { version = "0.32", features = ["bundled"] }
directories = "5"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }
tokio = { version = "1", features = ["full"] }
thiserror = "2"

# TTS dependencies
rodio = { version = "0.19", features = ["symphonia-all"] }
reqwest = { version = "0.12", features = ["blocking", "stream", "json"] }
zip = "2"
tempfile = "3"
futures-util = "0.3"
regex-lite = "0.1"
regex = "1"

# STT dependencies
base64 = "0.22"
hound = "3.5"

# Local LLM inference (embedded llama.cpp)
llama-cpp-2 = { version = "0.1", features = ["sampler"] }
async-trait = "0.1"
sha2 = "0.10"

# Ollama integration (fallback, kept for dev use via AILOCALMIND_USE_OLLAMA=1)
ollama-rs = "0.2"

# Encryption (ChaCha20-Poly1305)
chacha20poly1305 = "0.10"
zeroize = "1.7"
rand = "0.8"

# File parsing
pdf = "0.9"
encoding_rs = "0.8"

# Entity resolution & fuzzy matching
strsim = "0.11"

# GLiNER NER for PII detection (CPU-only, load-dynamic avoids CRT mismatch with llama-cpp-2)
gline-rs = { version = "1", default-features = false }
orp = "0.9"
# Force ort to load onnxruntime.dll at runtime (load-dynamic) instead of statically linking
# the prebuilt ONNX Runtime, which prevents LNK2038 RuntimeLibrary mismatch with esaxx-rs/llama-cpp-2.
ort = { version = "=2.0.0-rc.9", features = ["load-dynamic"] }

# Windows Credential Manager
winapi = { version = "0.3", features = ["wincred", "winerror"] }

# Additional utilities
log = "0.4"
env_logger = "0.11"

[patch.crates-io]
# esaxx-rs hardcodes .static_crt(true) in its build script, causing LNK2005/LNK1169
# when mixed with llama-cpp-sys-2 (/MD dynamic CRT). This patch changes it to /MD.
esaxx-rs = { path = "patches/esaxx-rs" }

